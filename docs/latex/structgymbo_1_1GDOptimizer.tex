\hypertarget{structgymbo_1_1GDOptimizer}{}\doxysection{gymbo\+::GDOptimizer Struct Reference}
\label{structgymbo_1_1GDOptimizer}\index{gymbo::GDOptimizer@{gymbo::GDOptimizer}}


Gradient Descent Optimizer for Symbolic Path Constraints.  




{\ttfamily \#include $<$gd.\+h$>$}

\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{structgymbo_1_1GDOptimizer_ac17fe6d6ef5d781ae590c0a8fb6e567b}{GDOptimizer}} (int \mbox{\hyperlink{structgymbo_1_1GDOptimizer_a7d5521480b4f536710d5a06897fa65bd}{num\+\_\+epochs}}=100, float \mbox{\hyperlink{structgymbo_1_1GDOptimizer_a69430f01307c0007f0db762fd437abfa}{lr}}=1.\+0, float \mbox{\hyperlink{structgymbo_1_1GDOptimizer_a8f024c3699cac041ced908ddf6ba8877}{eps}}=1.\+0, float \mbox{\hyperlink{structgymbo_1_1GDOptimizer_a7a81435ec290f50a98eaff343ab61d63}{param\+\_\+low}}=-\/10.\+0, float \mbox{\hyperlink{structgymbo_1_1GDOptimizer_ab14c4f9993f80b8e023a4c456c6e0b5f}{param\+\_\+high}}=10.\+0, bool \mbox{\hyperlink{structgymbo_1_1GDOptimizer_a1dab2d9895b58c4dda052a00cf8058ee}{sign\+\_\+grad}}=true, bool \mbox{\hyperlink{structgymbo_1_1GDOptimizer_a2a35f800649f9c57bc1f7d23f9f6110b}{init\+\_\+param\+\_\+uniform\+\_\+int}}=true, int \mbox{\hyperlink{structgymbo_1_1GDOptimizer_ab1631b9a0026ed9c12721df5900eb347}{seed}}=42)
\begin{DoxyCompactList}\small\item\em Constructor for \mbox{\hyperlink{structgymbo_1_1GDOptimizer}{GDOptimizer}}. \end{DoxyCompactList}\item 
bool \mbox{\hyperlink{structgymbo_1_1GDOptimizer_aabe2fc9e0528d282c105a1e723d75f02}{eval}} (std\+::vector$<$ \mbox{\hyperlink{structgymbo_1_1Sym}{Sym}} $>$ \&path\+\_\+constraints, std\+::unordered\+\_\+map$<$ int, float $>$ params)
\begin{DoxyCompactList}\small\item\em Evaluate if path constraints are satisfied for given parameters. \end{DoxyCompactList}\item 
bool \mbox{\hyperlink{structgymbo_1_1GDOptimizer_ab5737f4c5c901f4b0fe8effc4f8808da}{solve}} (std\+::vector$<$ \mbox{\hyperlink{structgymbo_1_1Sym}{Sym}} $>$ \&path\+\_\+constraints, std\+::unordered\+\_\+map$<$ int, float $>$ \&params, bool is\+\_\+init\+\_\+params\+\_\+const=true)
\begin{DoxyCompactList}\small\item\em Solve path constraints using gradient descent optimization. \end{DoxyCompactList}\end{DoxyCompactItemize}
\doxysubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
int \mbox{\hyperlink{structgymbo_1_1GDOptimizer_a7d5521480b4f536710d5a06897fa65bd}{num\+\_\+epochs}}
\begin{DoxyCompactList}\small\item\em Maximum number of optimization epochs. \end{DoxyCompactList}\item 
float \mbox{\hyperlink{structgymbo_1_1GDOptimizer_a69430f01307c0007f0db762fd437abfa}{lr}}
\begin{DoxyCompactList}\small\item\em Learning rate for gradient descent. \end{DoxyCompactList}\item 
float \mbox{\hyperlink{structgymbo_1_1GDOptimizer_a8f024c3699cac041ced908ddf6ba8877}{eps}}
\begin{DoxyCompactList}\small\item\em The smallest positive value. \end{DoxyCompactList}\item 
float \mbox{\hyperlink{structgymbo_1_1GDOptimizer_a7a81435ec290f50a98eaff343ab61d63}{param\+\_\+low}}
\begin{DoxyCompactList}\small\item\em Lower bound for parameter initialization. \end{DoxyCompactList}\item 
float \mbox{\hyperlink{structgymbo_1_1GDOptimizer_ab14c4f9993f80b8e023a4c456c6e0b5f}{param\+\_\+high}}
\begin{DoxyCompactList}\small\item\em Upper bound for parameter initialization. \end{DoxyCompactList}\item 
bool \mbox{\hyperlink{structgymbo_1_1GDOptimizer_a1dab2d9895b58c4dda052a00cf8058ee}{sign\+\_\+grad}}
\item 
bool \mbox{\hyperlink{structgymbo_1_1GDOptimizer_a2a35f800649f9c57bc1f7d23f9f6110b}{init\+\_\+param\+\_\+uniform\+\_\+int}}
\item 
bool \mbox{\hyperlink{structgymbo_1_1GDOptimizer_a76e7c59fa495bf56f25f26a84faf7112}{contain\+\_\+randomized\+\_\+vars}}
\item 
int \mbox{\hyperlink{structgymbo_1_1GDOptimizer_ab1631b9a0026ed9c12721df5900eb347}{seed}}
\begin{DoxyCompactList}\small\item\em Random seed for initializing parameter values. \end{DoxyCompactList}\item 
int \mbox{\hyperlink{structgymbo_1_1GDOptimizer_a3882db1f66c2141d2535cdef17b108f8}{num\+\_\+used\+\_\+itr}}
\begin{DoxyCompactList}\small\item\em Number of used iterations during optimization. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
Gradient Descent Optimizer for Symbolic Path Constraints. 

The {\ttfamily \mbox{\hyperlink{structgymbo_1_1GDOptimizer}{GDOptimizer}}} class provides functionality to optimize symbolic path constraints by using gradient descent. It aims to find parameter values that satisfy the given path constraints, making them true or non-\/positive.

\begin{DoxyNote}{Note}
This class assumes that symbolic expressions are represented by the {\ttfamily \mbox{\hyperlink{structgymbo_1_1Sym}{Sym}}} type, and path constraints are a vector of {\ttfamily \mbox{\hyperlink{structgymbo_1_1Sym}{Sym}}} objects.

The optimization process relies on the gradient information of the path constraints. The optimizer iteratively updates the parameters until the constraints are satisfied or a maximum number of epochs is reached. 
\end{DoxyNote}


\doxysubsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{structgymbo_1_1GDOptimizer_ac17fe6d6ef5d781ae590c0a8fb6e567b}\label{structgymbo_1_1GDOptimizer_ac17fe6d6ef5d781ae590c0a8fb6e567b}} 
\index{gymbo::GDOptimizer@{gymbo::GDOptimizer}!GDOptimizer@{GDOptimizer}}
\index{GDOptimizer@{GDOptimizer}!gymbo::GDOptimizer@{gymbo::GDOptimizer}}
\doxysubsubsection{\texorpdfstring{GDOptimizer()}{GDOptimizer()}}
{\footnotesize\ttfamily gymbo\+::\+GDOptimizer\+::\+GDOptimizer (\begin{DoxyParamCaption}\item[{int}]{num\+\_\+epochs = {\ttfamily 100},  }\item[{float}]{lr = {\ttfamily 1.0},  }\item[{float}]{eps = {\ttfamily 1.0},  }\item[{float}]{param\+\_\+low = {\ttfamily -\/10.0},  }\item[{float}]{param\+\_\+high = {\ttfamily 10.0},  }\item[{bool}]{sign\+\_\+grad = {\ttfamily true},  }\item[{bool}]{init\+\_\+param\+\_\+uniform\+\_\+int = {\ttfamily true},  }\item[{int}]{seed = {\ttfamily 42} }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Constructor for \mbox{\hyperlink{structgymbo_1_1GDOptimizer}{GDOptimizer}}. 


\begin{DoxyParams}{Parameters}
{\em num\+\_\+epochs} & Maximum number of optimization epochs (default\+: 100). \\
\hline
{\em lr} & Learning rate for gradient descent (default\+: 1). \\
\hline
{\em eps} & The smallest positive value of the target type (default\+: 1.\+0). \\
\hline
{\em param\+\_\+low} & Lower bound for parameter initialization (default\+: -\/10). \\
\hline
{\em param\+\_\+high} & Upper bound for parameter initialization (default\+: 10). \\
\hline
{\em sign\+\_\+grad} & If true, use sign gradient descent. Otherwise, use standard gradient descent. (default true). \\
\hline
{\em init\+\_\+param\+\_\+uniform\+\_\+int} & Flag indicating whether initial parameter values are drawn from the uniform int distribution or uniform real distribution (default true). \\
\hline
{\em seed} & Random seed for initializing parameter values (default\+: 42). \\
\hline
\end{DoxyParams}


\doxysubsection{Member Function Documentation}
\mbox{\Hypertarget{structgymbo_1_1GDOptimizer_aabe2fc9e0528d282c105a1e723d75f02}\label{structgymbo_1_1GDOptimizer_aabe2fc9e0528d282c105a1e723d75f02}} 
\index{gymbo::GDOptimizer@{gymbo::GDOptimizer}!eval@{eval}}
\index{eval@{eval}!gymbo::GDOptimizer@{gymbo::GDOptimizer}}
\doxysubsubsection{\texorpdfstring{eval()}{eval()}}
{\footnotesize\ttfamily bool gymbo\+::\+GDOptimizer\+::eval (\begin{DoxyParamCaption}\item[{std\+::vector$<$ \mbox{\hyperlink{structgymbo_1_1Sym}{Sym}} $>$ \&}]{path\+\_\+constraints,  }\item[{std\+::unordered\+\_\+map$<$ int, float $>$}]{params }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Evaluate if path constraints are satisfied for given parameters. 

This function checks if the given path constraints are satisfied (non-\/positive) for the provided parameter values.


\begin{DoxyParams}{Parameters}
{\em path\+\_\+constraints} & Vector of symbolic path constraints. \\
\hline
{\em params} & Map of parameter values. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
{\ttfamily true} if all constraints are satisfied; otherwise, {\ttfamily false}. 
\end{DoxyReturn}
\mbox{\Hypertarget{structgymbo_1_1GDOptimizer_ab5737f4c5c901f4b0fe8effc4f8808da}\label{structgymbo_1_1GDOptimizer_ab5737f4c5c901f4b0fe8effc4f8808da}} 
\index{gymbo::GDOptimizer@{gymbo::GDOptimizer}!solve@{solve}}
\index{solve@{solve}!gymbo::GDOptimizer@{gymbo::GDOptimizer}}
\doxysubsubsection{\texorpdfstring{solve()}{solve()}}
{\footnotesize\ttfamily bool gymbo\+::\+GDOptimizer\+::solve (\begin{DoxyParamCaption}\item[{std\+::vector$<$ \mbox{\hyperlink{structgymbo_1_1Sym}{Sym}} $>$ \&}]{path\+\_\+constraints,  }\item[{std\+::unordered\+\_\+map$<$ int, float $>$ \&}]{params,  }\item[{bool}]{is\+\_\+init\+\_\+params\+\_\+const = {\ttfamily true} }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Solve path constraints using gradient descent optimization. 

This function attempts to find parameter values that satisfy the given path constraints by using gradient descent optimization. It iteratively updates the parameters until the constraints are satisfied or the maximum number of epochs is reached.


\begin{DoxyParams}{Parameters}
{\em path\+\_\+constraints} & Vector of symbolic path constraints. \\
\hline
{\em params} & Map of parameter values (will be modified during optimization). \\
\hline
{\em is\+\_\+init\+\_\+params\+\_\+const} & Flag indicating whether initial parameter values are constant. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
{\ttfamily true} if the constraints are satisfied after optimization; otherwise, {\ttfamily false}. 
\end{DoxyReturn}


\doxysubsection{Member Data Documentation}
\mbox{\Hypertarget{structgymbo_1_1GDOptimizer_a76e7c59fa495bf56f25f26a84faf7112}\label{structgymbo_1_1GDOptimizer_a76e7c59fa495bf56f25f26a84faf7112}} 
\index{gymbo::GDOptimizer@{gymbo::GDOptimizer}!contain\_randomized\_vars@{contain\_randomized\_vars}}
\index{contain\_randomized\_vars@{contain\_randomized\_vars}!gymbo::GDOptimizer@{gymbo::GDOptimizer}}
\doxysubsubsection{\texorpdfstring{contain\_randomized\_vars}{contain\_randomized\_vars}}
{\footnotesize\ttfamily bool gymbo\+::\+GDOptimizer\+::contain\+\_\+randomized\+\_\+vars}

If true, use aeval and agrad. Otherwise, use eval and grad. \mbox{\Hypertarget{structgymbo_1_1GDOptimizer_a8f024c3699cac041ced908ddf6ba8877}\label{structgymbo_1_1GDOptimizer_a8f024c3699cac041ced908ddf6ba8877}} 
\index{gymbo::GDOptimizer@{gymbo::GDOptimizer}!eps@{eps}}
\index{eps@{eps}!gymbo::GDOptimizer@{gymbo::GDOptimizer}}
\doxysubsubsection{\texorpdfstring{eps}{eps}}
{\footnotesize\ttfamily float gymbo\+::\+GDOptimizer\+::eps}



The smallest positive value. 

\mbox{\Hypertarget{structgymbo_1_1GDOptimizer_a2a35f800649f9c57bc1f7d23f9f6110b}\label{structgymbo_1_1GDOptimizer_a2a35f800649f9c57bc1f7d23f9f6110b}} 
\index{gymbo::GDOptimizer@{gymbo::GDOptimizer}!init\_param\_uniform\_int@{init\_param\_uniform\_int}}
\index{init\_param\_uniform\_int@{init\_param\_uniform\_int}!gymbo::GDOptimizer@{gymbo::GDOptimizer}}
\doxysubsubsection{\texorpdfstring{init\_param\_uniform\_int}{init\_param\_uniform\_int}}
{\footnotesize\ttfamily bool gymbo\+::\+GDOptimizer\+::init\+\_\+param\+\_\+uniform\+\_\+int}

Flag indicating whether initial parameter values are drawn from the uniform int distribution or uniform real distribution (default true). \mbox{\Hypertarget{structgymbo_1_1GDOptimizer_a69430f01307c0007f0db762fd437abfa}\label{structgymbo_1_1GDOptimizer_a69430f01307c0007f0db762fd437abfa}} 
\index{gymbo::GDOptimizer@{gymbo::GDOptimizer}!lr@{lr}}
\index{lr@{lr}!gymbo::GDOptimizer@{gymbo::GDOptimizer}}
\doxysubsubsection{\texorpdfstring{lr}{lr}}
{\footnotesize\ttfamily float gymbo\+::\+GDOptimizer\+::lr}



Learning rate for gradient descent. 

\mbox{\Hypertarget{structgymbo_1_1GDOptimizer_a7d5521480b4f536710d5a06897fa65bd}\label{structgymbo_1_1GDOptimizer_a7d5521480b4f536710d5a06897fa65bd}} 
\index{gymbo::GDOptimizer@{gymbo::GDOptimizer}!num\_epochs@{num\_epochs}}
\index{num\_epochs@{num\_epochs}!gymbo::GDOptimizer@{gymbo::GDOptimizer}}
\doxysubsubsection{\texorpdfstring{num\_epochs}{num\_epochs}}
{\footnotesize\ttfamily int gymbo\+::\+GDOptimizer\+::num\+\_\+epochs}



Maximum number of optimization epochs. 

\mbox{\Hypertarget{structgymbo_1_1GDOptimizer_a3882db1f66c2141d2535cdef17b108f8}\label{structgymbo_1_1GDOptimizer_a3882db1f66c2141d2535cdef17b108f8}} 
\index{gymbo::GDOptimizer@{gymbo::GDOptimizer}!num\_used\_itr@{num\_used\_itr}}
\index{num\_used\_itr@{num\_used\_itr}!gymbo::GDOptimizer@{gymbo::GDOptimizer}}
\doxysubsubsection{\texorpdfstring{num\_used\_itr}{num\_used\_itr}}
{\footnotesize\ttfamily int gymbo\+::\+GDOptimizer\+::num\+\_\+used\+\_\+itr}



Number of used iterations during optimization. 

\mbox{\Hypertarget{structgymbo_1_1GDOptimizer_ab14c4f9993f80b8e023a4c456c6e0b5f}\label{structgymbo_1_1GDOptimizer_ab14c4f9993f80b8e023a4c456c6e0b5f}} 
\index{gymbo::GDOptimizer@{gymbo::GDOptimizer}!param\_high@{param\_high}}
\index{param\_high@{param\_high}!gymbo::GDOptimizer@{gymbo::GDOptimizer}}
\doxysubsubsection{\texorpdfstring{param\_high}{param\_high}}
{\footnotesize\ttfamily float gymbo\+::\+GDOptimizer\+::param\+\_\+high}



Upper bound for parameter initialization. 

\mbox{\Hypertarget{structgymbo_1_1GDOptimizer_a7a81435ec290f50a98eaff343ab61d63}\label{structgymbo_1_1GDOptimizer_a7a81435ec290f50a98eaff343ab61d63}} 
\index{gymbo::GDOptimizer@{gymbo::GDOptimizer}!param\_low@{param\_low}}
\index{param\_low@{param\_low}!gymbo::GDOptimizer@{gymbo::GDOptimizer}}
\doxysubsubsection{\texorpdfstring{param\_low}{param\_low}}
{\footnotesize\ttfamily float gymbo\+::\+GDOptimizer\+::param\+\_\+low}



Lower bound for parameter initialization. 

\mbox{\Hypertarget{structgymbo_1_1GDOptimizer_ab1631b9a0026ed9c12721df5900eb347}\label{structgymbo_1_1GDOptimizer_ab1631b9a0026ed9c12721df5900eb347}} 
\index{gymbo::GDOptimizer@{gymbo::GDOptimizer}!seed@{seed}}
\index{seed@{seed}!gymbo::GDOptimizer@{gymbo::GDOptimizer}}
\doxysubsubsection{\texorpdfstring{seed}{seed}}
{\footnotesize\ttfamily int gymbo\+::\+GDOptimizer\+::seed}



Random seed for initializing parameter values. 

\mbox{\Hypertarget{structgymbo_1_1GDOptimizer_a1dab2d9895b58c4dda052a00cf8058ee}\label{structgymbo_1_1GDOptimizer_a1dab2d9895b58c4dda052a00cf8058ee}} 
\index{gymbo::GDOptimizer@{gymbo::GDOptimizer}!sign\_grad@{sign\_grad}}
\index{sign\_grad@{sign\_grad}!gymbo::GDOptimizer@{gymbo::GDOptimizer}}
\doxysubsubsection{\texorpdfstring{sign\_grad}{sign\_grad}}
{\footnotesize\ttfamily bool gymbo\+::\+GDOptimizer\+::sign\+\_\+grad}

If true, use sign gradient descent. Otherwise, use standard gradient descent. (default true). 

The documentation for this struct was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
libgymbo/\mbox{\hyperlink{gd_8h}{gd.\+h}}\end{DoxyCompactItemize}
