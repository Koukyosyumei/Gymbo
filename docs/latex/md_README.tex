

Gymbo is a Proof of Concept for a Gradient-\/based Symbolic Execution Engine, implemented from scratch. Building on recent advancements that utilize gradient descent to solve SMT formulas {\ttfamily \mbox{[}1, 2\mbox{]}}, Gymbo leverages gradient descent to discover input values that fulfill each path constraint during symbolic execution.

Compared to other prominent symbolic execution tools, Gymbo\textquotesingle{}s implementation is notably simpler and more compact. We hope that this project will assist individuals in grasping the fundamental principles of symbolic execution and SMT problem-\/solving through gradient descent.

One practical usage of Gymbo is debugging ML models like neural networks to detect unexpected behaviors. For example, you can generate adversarial examples with Gymbo by converting neural networks to imperative programs.



\begin{quote}
Please note that Gymbo is currently under development and may have bugs. Your feedback and contributions are always greatly appreciated. \end{quote}
\hypertarget{md_README_autotoc_md1}{}\doxysection{Install}\label{md_README_autotoc_md1}

\begin{DoxyCode}{0}
\DoxyCodeLine{git clone https://github.com/Koukyosyumei/Gymbo.git}
\DoxyCodeLine{./build.sh}

\end{DoxyCode}
\hypertarget{md_README_autotoc_md2}{}\doxysection{Input Source Code Grammar}\label{md_README_autotoc_md2}
Gymbo presently supports C-\/like programs with the following BNF grammar\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{program    = stmt*}
\DoxyCodeLine{stmt       = expr "{};"{}}
\DoxyCodeLine{           | "{}\{"{} stmt* "{}\}"{}}
\DoxyCodeLine{           | "{}if"{} "{}("{} expr "{})"{} stmt ("{}else"{} stmt)? }
\DoxyCodeLine{           | "{}return"{} expr "{};"{}}
\DoxyCodeLine{expr       = assign}
\DoxyCodeLine{assign     = logical ("{}="{} assign)?}
\DoxyCodeLine{logical    = equality ("{}\&\&"{} equality | "{}||"{} equality)*}
\DoxyCodeLine{equality   = relational ("{}=="{} relational | "{}!="{} relational)*}
\DoxyCodeLine{relational = add ("{}<"{} add | "{}<="{} add | "{}>"{} add | "{}>="{} add)*}
\DoxyCodeLine{add        = mul ("{}+"{} mul | "{}-\/"{} mul)*}
\DoxyCodeLine{mul        = unary ("{}*"{} unary | "{}/"{} unary)*}
\DoxyCodeLine{unary      = ("{}+"{} | "{}-\/"{})? primary}
\DoxyCodeLine{primary    = num | ident | "{}("{} expr "{})"{}}

\end{DoxyCode}


\begin{quote}
Please note that Gymbo currently ignores {\ttfamily /} when solving path constraints. \end{quote}
\hypertarget{md_README_autotoc_md3}{}\doxysection{Internal Algorithm}\label{md_README_autotoc_md3}
Gymbo converts the path constraint into a numerical loss function, which becomes negative only when the path constraint is satisfied. Gymbo uses the following transformation rule\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{!(a)     => -\/a + eps}
\DoxyCodeLine{(a < b)  => a -\/ b + eps}
\DoxyCodeLine{(a <= b) => a -\/ b}
\DoxyCodeLine{(a > b)  => b -\/ a + eps}
\DoxyCodeLine{(a >= b) => b -\/ a}
\DoxyCodeLine{(a == b) => abs(a -\/ b)}
\DoxyCodeLine{(a != b) => -\/abs(a -\/ b) + eps}
\DoxyCodeLine{(a \&\& b) => max(0, a) + max(0, b)}
\DoxyCodeLine{(a || b) => max(0, a) * max(0, b)}

\end{DoxyCode}


Here, {\ttfamily eps} is the smallest positive value of the type for a and b.

For example, when {\ttfamily a} and {\ttfamily b} are integers ({\ttfamily eps = 1}), {\ttfamily (a $<$ 3) \&\& (!(a $<$ 3) $\vert$$\vert$ (b == 5))} becomes {\ttfamily (a -\/ 2) + (max(0, (3 -\/ a)) $\ast$ max(0, abs(b -\/ 5)))}.

Optionally, Gymbo can use DPLL (SAT solver) to decide the assignment for each unique term, sometimes resulting in better scalability. For example, applying DPLL to the above example leads to {\ttfamily (a $<$ 3)} being true and {\ttfamily (b == 5)} being true. Gymbo then converts this assignment into a loss function to be solved\+: {\ttfamily (a -\/ 2) + max(0, abs(b -\/ 5))}.\hypertarget{md_README_autotoc_md4}{}\doxysection{CLI Tool}\label{md_README_autotoc_md4}
{\ttfamily gymbo} command accepts the following command-\/line options\+:


\begin{DoxyItemize}
\item {\ttfamily -\/d}\+: Set the maximum depth for symbolic execution (default\+: 256).
\item {\ttfamily -\/v}\+: Set the verbosity level (default\+: 1). Use -\/1 for minimal output. 
\begin{DoxyCode}{0}
\DoxyCodeLine{-\/1: the number of satisfiable path constraints and unsatisfiable path constraints.}
\DoxyCodeLine{0: + the list of unique unsatisfiable path constraints.}
\DoxyCodeLine{1: + estimated concrete parameters for each path constraint.}
\DoxyCodeLine{2: + trace at each operation, including the content of the virtual stack and memory.}
\DoxyCodeLine{3: + complete stack machine.}

\end{DoxyCode}

\item {\ttfamily -\/i}\+: Set the number of iterations for gradient descent (default\+: 100).
\item {\ttfamily -\/a}\+: Set the step size for gradient descent (default\+: 1.\+0).
\item {\ttfamily -\/e}\+: Set the eps for the differentiable expression (default\+: 1.\+0).
\item {\ttfamily -\/t}\+: Set the maximum number of trials of gradient descent (default\+: 3)
\item {\ttfamily -\/l}\+: Set the minimum value of initial parameters (default\+: -\/10)
\item {\ttfamily -\/h}\+: Set the maximum value of initial parameters (default\+: 10)
\item {\ttfamily -\/s}\+: Set the random seed (default\+: 42)
\item {\ttfamily -\/p}\+: (optional) If set, use DPLL to determine the assignment for each term. Otherwise, solve the loss function directly transformed from the path constraints.
\end{DoxyItemize}


\begin{DoxyCode}{0}
\DoxyCodeLine{./gymbo "{}if (a < 3) if (a > 4) return 1;"{} -\/v 0}
\DoxyCodeLine{}
\DoxyCodeLine{>Compiling the input program...}
\DoxyCodeLine{>Start Symbolic Execution...}
\DoxyCodeLine{>-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/}
\DoxyCodeLine{>Result Summary}
\DoxyCodeLine{>\#Total Path Constraints: 4}
\DoxyCodeLine{>\#SAT: 3}
\DoxyCodeLine{>\#UNSAT: 1}
\DoxyCodeLine{>List of UNSAT Path Constraints}
\DoxyCodeLine{>\# var\_1 < 3 and 4 < var\_1 and  1}

\end{DoxyCode}


\hypertarget{md_README_autotoc_md5}{}\doxysection{$<$tt$>$libgymbo$<$/tt$>$\+: Header-\/only Library}\label{md_README_autotoc_md5}
Since gymbo consists of the header-\/only library, you can easily create your own symbolic execution tool.


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{preprocessor}{\#include "{}libgymbo/compiler.h"{}}}
\DoxyCodeLine{\textcolor{preprocessor}{\#include "{}libgymbo/gd.h"{}}}
\DoxyCodeLine{\textcolor{preprocessor}{\#include "{}libgymbo/parser.h"{}}}
\DoxyCodeLine{\textcolor{preprocessor}{\#include "{}libgymbo/tokenizer.h"{}}}
\DoxyCodeLine{\textcolor{preprocessor}{\#include "{}libgymbo/type.h"{}}}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{keywordtype}{char} user\_input[] = \textcolor{stringliteral}{"{}if (a < 3) return 1;"{}}}
\DoxyCodeLine{}
\DoxyCodeLine{std::vector<gymbo::Node *> code;}
\DoxyCodeLine{gymbo::Prog prg;}
\DoxyCodeLine{gymbo::GDOptimizer optimizer(num\_itrs, step\_size);}
\DoxyCodeLine{gymbo::SymState init;}
\DoxyCodeLine{gymbo::PathConstraintsTable cache\_constraints;}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{comment}{// tokenize the input source code}}
\DoxyCodeLine{gymbo::Token *token = gymbo::tokenize(user\_input);}
\DoxyCodeLine{\textcolor{comment}{// generate AST from the tokens}}
\DoxyCodeLine{gymbo::generate\_ast(token, user\_input, code);}
\DoxyCodeLine{\textcolor{comment}{// construct virtual stack machine from AST}}
\DoxyCodeLine{gymbo::compile\_ast(code, prg);}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{comment}{// execute gradient-\/based symbolie execution}}
\DoxyCodeLine{gymbo::run\_gymbo(prg, optimizer, init, cache\_constraints, ...);}

\end{DoxyCode}
\hypertarget{md_README_autotoc_md6}{}\doxysection{Python API}\label{md_README_autotoc_md6}
\hypertarget{md_README_autotoc_md7}{}\doxysubsection{Install}\label{md_README_autotoc_md7}

\begin{DoxyCode}{0}
\DoxyCodeLine{pip install git+https://github.com/Koukyosyumei/Gymbo}

\end{DoxyCode}
\hypertarget{md_README_autotoc_md8}{}\doxysubsection{$<$tt$>$pylibgymbo$<$/tt$>$\+: Python Wrapper for $<$tt$>$libgymbo$<$/tt$>$}\label{md_README_autotoc_md8}

\begin{DoxyCode}{0}
\DoxyCodeLine{import pylibgymbo as plg}
\DoxyCodeLine{}
\DoxyCodeLine{inp = "{}a = 1; if (a == 1) return 2;"{}}
\DoxyCodeLine{var\_counter, prg = plg.gcompile(inp)}
\DoxyCodeLine{}
\DoxyCodeLine{optimizer = plg.GDOptimizer(num\_itrs, step\_size, ...)}
\DoxyCodeLine{constraints = plg.gexecute(prg, init\_symstate, optimizer, ...)}

\end{DoxyCode}
\hypertarget{md_README_autotoc_md9}{}\doxysubsection{$<$tt$>$pymlgymbo$<$/tt$>$\+: Debugging Machine Learning Models}\label{md_README_autotoc_md9}
We offer some helper functions within {\ttfamily pymlgymbo} library to convert the ML models of famous Python library like sklearn and Py\+Torch to the program that gymbo can process.

The following code generates the adversarial examples against a neural network, as proposed in \mbox{[}3\mbox{]}


\begin{DoxyCode}{0}
\DoxyCodeLine{from sklearn.neural\_network import MLPClassifier}
\DoxyCodeLine{}
\DoxyCodeLine{import pylibgymbo as plg}
\DoxyCodeLine{import pymlgymbo as pmg}
\DoxyCodeLine{}
\DoxyCodeLine{clf = MLPClassifier(activation="{}relu"{})}
\DoxyCodeLine{clf.fit(X\_train, y\_train)}
\DoxyCodeLine{}
\DoxyCodeLine{}
\DoxyCodeLine{mlp\_code = pmg.dump\_sklearn\_MLP(clf, feature\_names)}
\DoxyCodeLine{adv\_condition = (}
\DoxyCodeLine{        "{}("{}}
\DoxyCodeLine{        + "{} || "{}.join(}
\DoxyCodeLine{            [f"{}(y\_\{c\} > y\_\{y\_pred\})"{} for c in range(len(clf.classes\_)) if y\_pred != c]}
\DoxyCodeLine{        )}
\DoxyCodeLine{        + "{})"{}}
\DoxyCodeLine{    )}
\DoxyCodeLine{}
\DoxyCodeLine{optimizer = plg.GDOptimizer(num\_itrs, step\_size, ...)}
\DoxyCodeLine{var\_counter, prg = plg.gcompile(mlp\_code)}
\DoxyCodeLine{constraints = plg.gexecute(prg, init\_symstate, optimizer, target\_pcs, ...)}

\end{DoxyCode}
\hypertarget{md_README_autotoc_md10}{}\doxysection{Acknowledgement}\label{md_README_autotoc_md10}
Gymbo is entirely implemented in C++ and requires only standard libraries. The process of compiling from source code to stack machines is based on the implementation of {\ttfamily rui314/chibicc \mbox{[}4\mbox{]}}, while the symbolic execution approach is inspired by {\ttfamily beala/symbolic \mbox{[}5\mbox{]}}.\hypertarget{md_README_autotoc_md11}{}\doxysection{Reference}\label{md_README_autotoc_md11}

\begin{DoxyCode}{0}
\DoxyCodeLine{-\/ [1] Chen, Peng, Jianzhong Liu, and Hao Chen. "{}Matryoshka: fuzzing deeply nested branches."{} Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. 2019.}
\DoxyCodeLine{-\/ [2] Minghao Liu, Kunhang Lv, Pei Huang, Rui Han, Fuqi Jia, Yu Zhang, Feifei Ma, Jian Zhang. "{}NRAgo: Solving SMT(NRA) Formulas with Gradient-\/based Optimization."{} IEEE/ACM International Conference on Automated Software Engineering, 2023}
\DoxyCodeLine{-\/ [3] Gopinath, Divya, et al. "{}Symbolic execution for importance analysis and adversarial generation in neural networks."{} 2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE). IEEE, 2019.}
\DoxyCodeLine{-\/ [4] https://github.com/rui314/chibicc}
\DoxyCodeLine{-\/ [5] https://github.com/beala/symbolic}

\end{DoxyCode}
 